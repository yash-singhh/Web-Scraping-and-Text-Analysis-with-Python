{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "0yHJm8Ni3VHJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRI1AonoozQW",
        "outputId": "b42822d4-f12b-4fb2-b724-4c782a745658"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the input data\n",
        "input_file = pd.read_csv(\"/content/Input.csv\")\n",
        "output_file = pd.read_csv(\"/content/Output Data Structure.csv\")"
      ],
      "metadata": {
        "id": "NUk6kxC9o4Lq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove unwanted content\n",
        "        for footer in soup.find_all(class_=\"td-footer-template-wrap\"):\n",
        "            footer.decompose()\n",
        "        for header in soup.find_all(class_=\"td-header-template-wrap\"):\n",
        "            header.decompose()\n",
        "        for img in soup.find_all('img'):\n",
        "            img.decompose()\n",
        "        for href in soup.find_all('href'):\n",
        "            href.decompose()\n",
        "\n",
        "        # Extract text\n",
        "        text = \"\"\n",
        "        for paragraph in soup.find_all('p'):\n",
        "            text += paragraph.get_text() + \" \"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {url}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "jMJzmwgmpGkf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute text analysis variables\n",
        "def compute_text_analysis(text):\n",
        "    # Tokenize text into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Define stopwords and initialize stemmer\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    # Initialize variables\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "    total_syllables = 0\n",
        "    complex_word_count = 0\n",
        "    personal_pronouns = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
        "    personal_pronoun_count = 0\n",
        "\n",
        "    # Compute variables\n",
        "    for word in words:\n",
        "        # Syllable count\n",
        "        total_syllables += syllable_count(word)\n",
        "\n",
        "        # Complex word count\n",
        "        if syllable_count(word) > 2:\n",
        "            complex_word_count += 1\n",
        "\n",
        "        # Personal pronoun count\n",
        "        if word.lower() in personal_pronouns:\n",
        "            personal_pronoun_count += 1\n",
        "\n",
        "    # Compute average sentence length\n",
        "    avg_sentence_length = total_words / total_sentences\n",
        "\n",
        "    # Compute percentage of complex words\n",
        "    percentage_complex_words = (complex_word_count / total_words) * 100\n",
        "\n",
        "    # Compute FOG index\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Compute average number of words per sentence\n",
        "    avg_words_per_sentence = total_words / total_sentences\n",
        "\n",
        "    # Compute syllables per word\n",
        "    syllables_per_word = total_syllables / total_words\n",
        "\n",
        "    # Compute polarity and subjectivity scores\n",
        "    blob = TextBlob(text)\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "    # Compute positive and negative score\n",
        "    positive_score = sum([1 for word in words if word.lower() in positive_words])\n",
        "    negative_score = sum([1 for word in words if word.lower() in negative_words])\n",
        "\n",
        "    # Compute average word length\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
        "            percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count,\n",
        "            total_words, syllables_per_word, personal_pronoun_count, avg_word_length]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmLlHeoApLwy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count syllables in a word\n",
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    if word in ['a', 'i', 'u', 'e', 'o']:\n",
        "        return 1\n",
        "    if word[:3] == \"tri\" and len(word) > 4 and word[:4] != \"tria\":\n",
        "        return len(re.findall('(?!e$)[aeiouy]+', word)) + 1\n",
        "    return len(re.findall('(?!e$)[aeiouy]+', word))"
      ],
      "metadata": {
        "id": "2tjaKNwMpQBq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = set(pd.read_csv(\"/content/positive-words.txt\", header=None, encoding='ISO-8859-1')[0])\n",
        "negative_words = set(pd.read_csv(\"/content/negative-words.txt\", header=None, encoding='ISO-8859-1')[0])"
      ],
      "metadata": {
        "id": "NW1vU0QqpVz0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with NaN values in the 'URL' column\n",
        "input_data = input_data.dropna(subset=['URL'])\n",
        "\n",
        "# Extract text and compute variables for each URL\n",
        "output_data = []\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    text = extract_text_from_url(url)\n",
        "    if text:\n",
        "        variables = compute_text_analysis(text)\n",
        "        output_data.append([url_id] + variables)\n",
        "    else:\n",
        "        output_data.append([url_id] + [None] * 13)\n"
      ],
      "metadata": {
        "id": "bRxCZKlfvCwS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text and compute variables for each URL\n",
        "output_data = []\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    text = extract_text_from_url(url)\n",
        "    if text:\n",
        "        variables = compute_text_analysis(text)\n",
        "        output_data.append([url_id] + variables)\n",
        "    else:\n",
        "        output_data.append([url_id] + [None] * 13)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1WRbyBIpXZo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for output data\n",
        "output_columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "                  'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "                  'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
        "                  'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "output_df = pd.DataFrame(output_data, columns=output_columns)"
      ],
      "metadata": {
        "id": "-Wsx7MgXpZ9z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path for the output CSV file\n",
        "output_csv_file_path = \"/content/Output Data Structure.csv\"\n",
        "\n",
        "# Save output DataFrame to CSV file\n",
        "output_df.to_csv(output_csv_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "rOvNEk3OpbKo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ot90bhXlwwZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}